Date: 8th May 2025
Date Modified: 8th May 2025
File Folder: Kanban
## Publication Information

**Database:** Elsevier

**DOI**: https://www.sciencedirect.com/science/article/pii/S2667102623000578

**Authors**: Hanhui Xu, Kyle Michael James Shuttleworth

**Publication Year**: 2024

**Country of Study**: USA

**Tags**: #medical #ai #blackbox #donoharm #paternalism

```ad-abstract
title: Abstract
collapse: open
One concern about the application of medical artificial intelligence (AI) regards the “black box” feature which can only be viewed in terms of its inputs and outputs, with no way to understand the AI's algorithm. This is problematic because patients, physicians, and even designers, do not understand why or how a treatment recommendation is produced by AI technologies. One view claims that the worry about black-box medicine is unreasonable because AI systems outperform human doctors in identifying the disease. Furthermore, under the medical AI-physician-patient model, the physician can undertake the responsibility of interpreting the medical AI's diagnosis. In this study, we focus on the potential harm caused by the unexplainability feature of medical AI and try to show that such possible harm is underestimated. We will seek to contribute to the literature from three aspects. First, we appealed to a thought experiment to show that although the medical AI systems perform better on accuracy, the harm caused by medical AI's misdiagnoses may be more serious than that caused by human doctors’ misdiagnoses in some cases. Second, in patient-centered medicine, physicians were obligated to provide adequate information to their patients in medical decision-making. However, the unexplainability feature of medical AI systems would limit the patient's autonomy. Last, we tried to illustrate the psychological and financial burdens that may be caused by the unexplainablity feature of medical AI systems, which seems to be ignored by the previous ethical discussions.
```

**Embed to Paper**: ![[Medical artificial intelligence and the black box problem - a view based on the ethical principle of do no harm.pdf]]
## Summary

### **I. Introduction**

- The global medical AI market is growing rapidly (projected to reach $187.76 billion by 2031).
    
- Medical AI, especially **deep learning (DL)** systems, operates as a “black box” — users can see inputs and outputs but not how results are generated.
    
- Some argue that this is acceptable because AI outperforms human doctors in diagnostic accuracy.
    
- This paper argues otherwise: **unexplainability introduces serious, underestimated harm**.
    

---

### **II. The Black Box Problem**

#### Key Characteristics of Medical AI Systems:

1. **Self-learning capacity** via artificial neural networks (ANNs) inspired by the human brain.
    
2. **High diagnostic accuracy**, often surpassing human experts (e.g., DeepMind's breast cancer model, Deep Patient's psychiatric prediction).
    
3. **Unexplainability** — even developers don’t know how the AI reaches specific conclusions.
    

- Example: Deep Patient accurately predicts schizophrenia without revealing **how**.
    
- **Watson (IBM)**: High-performance but opaque; this limits the ability to detect errors and undermines **clinical trust**.
    

---

### **III. Medical Malpractice and Medical AI**

- Medical ethics are grounded in **“do no harm”** — interpreted as the obligation to avoid **unnecessary** harm.
    
- Two types of harm:
    
    1. From **misdiagnoses/treatment errors**
        
    2. From **paternalism** — decisions made without the patient’s knowledge or consent.
        

#### Table 1 – Notable Harmful Cases Involving AI:

|Year|System|Problem|
|---|---|---|
|2016|Tay (Twitter bot)|Became racist due to biased training|
|2018|Uber self-driving car|Killed a pedestrian|
|2020|Facebook facial recognition|Labeled Black men as "primates"|
|2020|Dutch SyRI|Used biased welfare fraud predictions|
